{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWdUnHieml6Ni0YJf+qMHX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Problem :\n",
        "\n",
        "Go to this url: https://www.flipkart.com/search?q=smartphones. This is the url to find phones in flipkart website. You have to extract the below things:\n",
        "1. image url of the phone\n",
        "2. name of the image\n",
        "3. average ratings\n",
        "4. total ratings\n",
        "5. total reviews\n",
        "6. discounted price\n",
        "7. actual price\n",
        "\n",
        "Extract all the phones which are available in this website. So you have to use the pagination concept. **Also after requesting every page through the url, please wait for a while (minimum 2-3 seconds), otherwise your IP address can be banned to access the flipkart website later.**\n",
        "\n",
        "After collecting all the data, save that in a JSON file."
      ],
      "metadata": {
        "id": "V514SPZd8f63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "iqSEm11Wd9tE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "x9jJr4NjeBjM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage = requests.get('https://www.flipkart.com/search?q=smartphones&page=1').text"
      ],
      "metadata": {
        "id": "jfKTD0CWeMQ3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(webpage,'lxml')"
      ],
      "metadata": {
        "id": "0vCVjOPleMyU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_divs = soup.find_all('div', class_=\"tUxRFH\")"
      ],
      "metadata": {
        "id": "gZSoqx5TeM1M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_divs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqv0xPYci2nA",
        "outputId": "d95dd096-0238-4359-ae1d-eccbd3e4cb3c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.DataFrame()\n",
        "\n",
        "for i in range(1,6):\n",
        "\n",
        "  webpage = requests.get('https://www.flipkart.com/search?q=smartphones&page={}'.format(i)).text\n",
        "  soup = BeautifulSoup(webpage,'lxml')\n",
        "  all_divs = soup.find_all('div', class_=\"tUxRFH\")\n",
        "\n",
        "  phone_image_url = []\n",
        "  phone_name = []\n",
        "  phone_rating = []\n",
        "  phone_num_ratings = []\n",
        "  phone_price = []\n",
        "\n",
        "  for phone in all_divs:\n",
        "    try:\n",
        "      phone_image_url.append(phone.find(\"img\",class_=\"DByuf4\")['src'])\n",
        "    except:\n",
        "      phone_image_url.append(np.nan)\n",
        "\n",
        "    try:\n",
        "      phone_name.append(phone.find(\"div\",class_=\"KzDlHZ\").text.strip())\n",
        "    except:\n",
        "      phone_name.append(np.nan)\n",
        "\n",
        "    try:\n",
        "      phone_rating.append(phone.find(\"div\",class_=\"XQDdHH\").text.strip())\n",
        "    except:\n",
        "      phone_rating.append(np.nan)\n",
        "\n",
        "    try:\n",
        "      phone_num_ratings.append(phone.find_all('span',class_='Wphh3N')[0].find_all('span')[0].find_all('span')[0].text.strip())\n",
        "    except:\n",
        "      phone_num_ratings.append(np.nan)\n",
        "\n",
        "    try:\n",
        "      phone_price.append(phone.find(\"div\",class_=\"Nx9bqj _4b5DiR\").text.strip())\n",
        "    except:\n",
        "      phone_price.append(np.nan)\n",
        "\n",
        "\n",
        "  df = pd.DataFrame({'phone_name':phone_name,'phone_price':phone_price,'phone_rating':phone_rating,'phone_num_rating':phone_num_ratings,'phone_img_url':phone_image_url})\n",
        "  final_df = pd.concat([final_df, df], ignore_index=True)\n",
        "\n",
        "  phone_image_url.clear()\n",
        "  phone_name.clear()\n",
        "  phone_rating.clear()\n",
        "  phone_num_ratings.clear()\n",
        "  phone_price.clear()\n",
        "\n",
        "  time.sleep(2)"
      ],
      "metadata": {
        "id": "k6Wh1xgWeM4E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKkDFPi6eM_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem\n",
        "\n",
        "Go to the site: https://rapidapi.com/wirefreethought/api/geodb-cities. From here, you have to grab the API and have to choose proper routes to get the cities of different countries. After getting the right API, hit that API and create a dataframe of all the cities that you can get by using the API. Then store the dataframe to a SQL. If you need to create an account or have to subscribe, then do that (it has free subscription but has some limitations. Use that free subscription and modify your accordingly to get all the data).  "
      ],
      "metadata": {
        "id": "ixwn5tD08dDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Rapid API\n",
        "import requests\n",
        "\n",
        "url = \"https://wft-geo-db.p.rapidapi.com/v1/geo/adminDivisions\"\n",
        "\n",
        "headers = {\n",
        "    \"X-RapidAPI-Key\": \"b0f75c828fmsh2b0480a62ab8d23p18178ajsn3267e033ae6a\",\n",
        "    \"X-RapidAPI-Host\": \"wft-geo-db.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "response = requests.request(\"GET\", url, headers=headers)\n",
        "\n",
        "data = response.json()['data']\n",
        "links = response.json()['links']\n",
        "metadata = response.json()['metadata']"
      ],
      "metadata": {
        "id": "w8Z9rOALeNHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_url = links[1]['href']"
      ],
      "metadata": {
        "id": "QJXusG59jxlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = metadata['totalCount']//5"
      ],
      "metadata": {
        "id": "6Scr2Ccej0F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "for i in range(1,count):\n",
        "\n",
        "    num_iters += 1\n",
        "\n",
        "    url = \"https://wft-geo-db.p.rapidapi.com\"\n",
        "    page = url + next_url\n",
        "\n",
        "    response = requests.request(\"GET\", page, headers=headers)\n",
        "\n",
        "    print(response.json())\n",
        "\n",
        "    links = response.json()['links']\n",
        "\n",
        "    for item in links:\n",
        "        if item['rel'] == 'next':\n",
        "            next_url = item['href']\n",
        "\n",
        "    data.extend(response.json()['data'])\n",
        "\n",
        "    time.sleep(2)"
      ],
      "metadata": {
        "id": "dYMwa4ivj2fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cities = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "UuZlFw3Dj7zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql"
      ],
      "metadata": {
        "id": "20QoHBJMj9l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine(\"mysql+pymysql://root:@localhost/temp_db\")\n",
        "#{root}:{Password}@{url}{dbname}\n",
        "\n",
        "cities.to_sql(\"cities\",con=engine,if_exists='append')"
      ],
      "metadata": {
        "id": "Vgzz8qeTkELm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}